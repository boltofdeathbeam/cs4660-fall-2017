{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agenda:\n",
    "### Probabilistic reasoning\n",
    "Bayes theorem\n",
    "    - Cancer test\n",
    "    - Scientific Method: Evolution by natural selection\n",
    "Naive Bayes\n",
    "    - Hidden features: get them from observed features\n",
    "    - Process: Get frequencies of observable features for each class of hidden\n",
    "    features\n",
    "        - Requires a training set where the hidden features are known\n",
    "    - To make a prediction for a new example, given its observable features,\n",
    "    see which hidden class, given the frequencies observed for these features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability of a hypothesis:\n",
    "\n",
    "$$ Pr(h|D) $$\n",
    "\n",
    "where $h$ is the hypothesis and $D$ is the data.\n",
    "\n",
    "Often $h$ is a class assignment to an example.\n",
    "\n",
    "Better:\n",
    "\n",
    "$$argmax_{h\\in H} Pr(h|D)$$\n",
    "\n",
    "Where $H$ is a set of hypotheses: we want the $h$ with the higest probability given our data.\n",
    "\n",
    "### Bayes's Theorem\n",
    "\n",
    "Sometimes it's easier to know the probability of the data given a hypothesis (rather than the other way around). That is, sometimes we know this:\n",
    "\n",
    "$$Pr(D|h)$$\n",
    "\n",
    "Example: We don't know whether Feona or Ginger wrote an comment on YouTube, but we know that Feona curses a lot, and Ginger is much more polite. Suppose our data is this comment:\n",
    "\n",
    "<span>\"This is so wack! You all need to ##$$@ yourself, #$@@$##!\" </span>\n",
    "\n",
    "Who do you think wrote this? That is, which is:\n",
    "higher: $Pr(Feona|D)$ or $Pr(Ginger|D)$?\n",
    "\n",
    "\n",
    "One thing is sure, we'd expect Feona to say such things, but not Ginger. That is, we know:\n",
    "$Pr(D|Feona) > Pr(D|Ginger)$\n",
    "\n",
    "\n",
    "Thus it would be nice to have something that relates the probabilities $Pr(Feona|D)$, $Pr(Ginger|D)$ to $Pr(D|Feona)$, $Pr(D|Ginger)$. Or, more generally:\n",
    "\n",
    "$$Pr(h|D) \\sim Pr(D|h)$$\n",
    "\n",
    "And we do, Baye's Theorem. We get it through these rules:\n",
    "\n",
    "$$Pr(A . B) = Pr(A|B)Pr(B)$$\n",
    "and\n",
    "$$Pr(B . A) = Pr(B|A)Pr(A)$$\n",
    "and \n",
    "$$Pr(A . B) = Pr(B . A)$$\n",
    "\n",
    "Doing some algebra we get Bayes's Theorem:\n",
    "\n",
    "$$Pr(A|B) = \\frac{Pr(B|A)Pr(A)}{Pr(B)}$$\n",
    "\n",
    "Or, using h and D:\n",
    "    $$ Pr(h|D) = \\frac{Pr(D|h)Pr(h)}{Pr(D)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to use this in an example:\n",
    "\n",
    "![](imgs/Bayes1.png)\n",
    "![](imgs/Bayes2.png)\n",
    "![](imgs/Bayes3.png)\n",
    "![](imgs/Bayes4.png)\n",
    "![](imgs/Bayes5.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Learning\n",
    "\n",
    "Algorithm:\n",
    "For each $h \\in H$ calculate \n",
    "   $$ Pr(h|D) = \\frac{Pr(D|h)Pr(h)}{Pr(D)}$$\n",
    "    \n",
    "and find the highest $h$.\n",
    "\n",
    "A good thing: we dont' need $Pr(D)$, since we are comparing hypotheses using the same D. We won't get a true probability, but we'll know which $h$ is best.\n",
    "\n",
    "\n",
    "So we have: \n",
    "\n",
    "## $$argmax_{h\\in H} Pr(h)Pr(D|h)$$\n",
    "\n",
    "Where $Pr(D|h)$ equals the product of all the probabilities of each of the observations $d$ given h. That is:\n",
    "\n",
    "$$ \\prod_{d \\in D}Pr(d|h)$$\n",
    "\n",
    "\n",
    "Question: What's naive about naive bayes? Look at the last formula. To find the probability of a conjunction of features, you simply multiply the probabilities of the features only if you know they are independent: one's probability doesn't affect any of the other's probability. We don't know that, and, in fact, we are probably wrong to assume it. But we do, and it empirically works out.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try naive bayes!!\n",
    "We'll use sklearn's implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n",
      "(150,)\n",
      "Accuracy on training data: 0.94\n",
      "Accuracy on test data: 0.98\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import numpy as np\n",
    "\n",
    "gnb = GaussianNB()\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "\n",
    "gnb.fit(X_train, y_train)\n",
    "acc_train = gnb.score(X_train, y_train)\n",
    "acc_test = gnb.score(X_test, y_test)\n",
    "\n",
    "print(\"Accuracy on training data:\", acc_train)\n",
    "print(\"Accuracy on test data:\", acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try tome text classification. The code below is taken from the sklearn documentation examples: Working with Text Data: http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html#loading-the-20-newsgroups-dataset\n",
    "\n",
    "The data:\n",
    "The 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups. To the best of our knowledge, it was originally collected by Ken Lang, probably for his paper “Newsweeder: Learning to filter netnews,” though he does not explicitly mention this collection. The 20 newsgroups collection has become a popular data set for experiments in text applications of machine learning techniques, such as text classification and text clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Limiting ourselves to 4 categories to try to predict\n",
    "categories = ['alt.atheism', 'soc.religion.christian',\n",
    "              'comp.graphics', 'sci.med']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    }
   ],
   "source": [
    "#downloading the dat\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "twenty_train = fetch_20newsgroups(subset='train',\n",
    "    categories=categories, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next bunch of cells is about preprocessing the data to vectors of word frequencies. I will just refer to the discussion in the link above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: sd345@city.ac.uk (Michael Collier)\n",
      "Subject: Converting images to HP LaserJet III?\n",
      "Nntp-Posting-Host: hampton\n",
      "comp.graphics\n"
     ]
    }
   ],
   "source": [
    "twenty_train.target_names\n",
    "print(\"\\n\".join(twenty_train.data[0].split(\"\\n\")[:3]))\n",
    "print(twenty_train.target_names[twenty_train.target[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 3, 3, 3, 3, 3, 2, 2, 2], dtype=int32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train.target[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comp.graphics\n",
      "comp.graphics\n",
      "soc.religion.christian\n",
      "soc.religion.christian\n",
      "soc.religion.christian\n",
      "soc.religion.christian\n",
      "soc.religion.christian\n",
      "sci.med\n",
      "sci.med\n",
      "sci.med\n"
     ]
    }
   ],
   "source": [
    "for t in twenty_train.target[:10]:\n",
    "    print(twenty_train.target_names[t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2257, 35788)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(twenty_train.data)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4690"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect.vocabulary_.get(u'algorithm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2257, 35788)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\n",
    "X_train_tf = tf_transformer.transform(X_train_counts)\n",
    "X_train_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2257, 35788)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train_tfidf, twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'God is love' => soc.religion.christian\n",
      "'OpenGL on the GPU is fast' => comp.graphics\n"
     ]
    }
   ],
   "source": [
    "docs_new = ['God is love', 'OpenGL on the GPU is fast']\n",
    "X_new_counts = count_vect.transform(docs_new)\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    "predicted = clf.predict(X_new_tfidf)\n",
    "\n",
    "for doc, category in zip(docs_new, predicted):\n",
    "    print('%r => %s' % (doc, twenty_train.target_names[category]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', MultinomialNB()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.fit(twenty_train.data, twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83488681757656458"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_test = fetch_20newsgroups(subset='test',\n",
    "    categories=categories, shuffle=True, random_state=42)\n",
    "docs_test = twenty_test.data\n",
    "predicted = text_clf.predict(docs_test)\n",
    "np.mean(predicted == twenty_test.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "           alt.atheism       0.97      0.60      0.74       319\n",
      "         comp.graphics       0.96      0.89      0.92       389\n",
      "               sci.med       0.97      0.81      0.88       396\n",
      "soc.religion.christian       0.65      0.99      0.78       398\n",
      "\n",
      "           avg / total       0.88      0.83      0.84      1502\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[192,   2,   6, 119],\n",
       "       [  2, 347,   4,  36],\n",
       "       [  2,  11, 322,  61],\n",
       "       [  2,   2,   1, 393]], dtype=int64)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(twenty_test.target, predicted,\n",
    "    target_names=twenty_test.target_names))\n",
    "\n",
    "metrics.confusion_matrix(twenty_test.target, predicted)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
